\section{Test Adequacy and Adaptation Quality}
As developers using TBSM express more concern over overfitting than any other problem, in this section we discuss overfitting as a result of test inadequacy and our attempts to solve it for the case study scenarios.

\subsection{Test Inadequacy}
Because resource adaptations generated by TBSM are test-adequate, if the tests are inadequate, we are bound to produce sub-standard adaptations. The primary problem with test inadequacy is, as we can see, overfitting: \note{Alex: Simplify rest of the statement, too difficult to read for someone who is not familiar with our work.}when there is no test for a feature that is not targeted for removal, or when the only tests for such a feature are mixed with a test for a feature that is to be sacrificed, TBSM has no way to distinguish this case from the code for that feature being part of the sacrificed functionality. Based on our observation of overfitting modifications in our case studies, we propose some basic guidelines for quickly improving a test suite to help TBSM distinguish between true and false modifications. 

\subsection{Code Coverage} 
TBSM will remove any code that is not covered by any tests (not removed from the suite) and not required for compilation. The easiest way to avoid this problem is to start with a high-coverage test suite. Developers can target poorly covered code with manual tests or use targeted test generation tools to cover such code. Improving code coverage using random testing reduced overfitting by 23\% and 8\% for \textit{Elevation API} and \textit{NetBeans IDE} respectively. Na\"ive test generation will not necessarily improve matters, however. Developers have to label any generated tests, and if tests mix multiple features, there still may often be cases where a feature not to be removed is never tested in isolation. We propose using delta-debugging to help isolate features in generated test cases~\cite{stvrcausereduce}: if a generated test covers two features, perhaps one can be removed by reducing the test with respect to targeted code for one feature only.

\subsection{Better Tests} 
Based on our study (with the developers) of overfitting, we identified a few categories covering most of the incorrect removals. For the \textit{Elevation API}, there were no tests for logging, and so all LOG statements were removed. This can be easily remedied by explicitly testing system logging as a feature. Similarly, exception-handling code was often inadequately tested. We worked with the developers to produce a new test covering each major overfitting category identified. We produced just 3 and 4 tests, respectively, for the \textit{Elevation API} and \textit{NetBeans IDE}, resulting in 20\% and 66\% reduction in overfitting. Identifying overfitting categories can be a highly context-specific task, but we suspect new automated test generation methods targeting specific under-tested categories such as logging and exception handling may be useful here.






