\section{Test Adequacy and Adaptation Quality}
evelopers using TBSM expressed more concern over overfitting than any other problem.  In this section we note the origins of this problem, and propose that solving it is synergistic with addressing a long-standing problem in software development.

\subsection{Test Inadequacy}

TBMS only knows what the tests tell it.  If tests are grossly inadequate, it is bound to produce sub-standard adaptations; in particular, TBSM is aggressive, and if a test does not force the inclusion of code in the adapted program, TBSM is likely to remove that functionality.  There are two basic sources of inadequacy.  One is true inadequacy, where the feature that TBSM ``accidentaly'' removes is simply not tested at all.  The solution to this problem is to test all features of a system that are of any actual importance, a worthy goal in any case.  The second problem is that there may be some tests for a feature, but they are all mixed-in with tests for a feature that is to be removed.  That is, the tests for a system are not well decomposed, and some functionality is only tested in conjunction with a feature that a developer wants to ``adapt away.''  Fortunately, these problems can be addressed by taking a better approach to tests in general.

\subsection{Code Coverage} 
TBSM will remove any code that is not covered by any tests, and not required for compilation. The easiest way to avoid this problem is to start with a high-coverage, high-quality test suite. Developers can target poorly covered code with manual tests or use targeted test generation tools to cover such code. Improving code coverage using random testing reduced overfitting by 23\% and 8\% for \textit{Elevation API} and \textit{NetBeans IDE} respectively. Na\"ive test generation will not necessarily improve matters, however. Developers have to label any generated tests, and if tests mix multiple features, there still may often be cases where a feature not to be removed is never tested in isolation. We propose using delta-debugging, in particular \emph{cause reduction} to help isolate features in generated test cases~\cite{stvrcausereduce}: if a generated test covers two features, perhaps one can be removed by reducing the test with respect to targeted code for one feature only.  In fact, in principle, we can automatically take each individual line of code, and produce a tests whose only purpose is to cover that line of code, with any other behavior required to retain that coverage.  If developers can then label files or functions in a program by functionality, the targeted code's location can automatically provide a label.

\subsection{Better Tests} 
Based on our study (with the developers) of overfitting, we identified a few categories covering most of the incorrect removals. For the \textit{Elevation API}, there were no tests for logging, and so all {\tt LOG} statements were removed. This problem is likely very common, but can be easily remedied by explicitly testing system logging as a feature.  Testing the logging output for performed actions in the tests for the functionality is a seldom-performed, but likely beneficial practice.  Similarly, exception-handling code was often inadequately tested.

We worked with the developers to produce a new test covering each major overfitting category identified. We produced just 3 and 4 tests, respectively, for the \textit{Elevation API} and \textit{NetBeans IDE} scenarios, resulting in 20\% and 66\% reduction in overfitting. Identifying overfitting categories can be a highly context-specific task, but we suspect new automated test generation methods targeting specific under-tested categories such as logging and exception handling may be useful here.






