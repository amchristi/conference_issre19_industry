\section{Test Adequacy and Adaptation Quality}
evelopers using TBSM expressed more concern over overfitting than any other problem.  In this section we note the origins of this problem, and propose that solving it is synergistic with addressing a long-standing problem in software development.

\subsection{Test Inadequacy}

TBMS only knows what the tests tell it.  If tests are grossly inadequate, it is bound to produce sub-standard adaptations; in particular, TBSM is aggressive, and if a test does not force the inclusion of code in the adapted program, TBSM is likely to remove that functionality.  There are two basic sources of inadequacy.  One is true inadequacy, where the feature that TBSM ``accidentaly'' removes is simply not tested at all.  The solution to this problem is to test all features of a system that are of any actual importance, a worthy goal in any case.  The second problem is that there may be some tests for a feature, but they are all mixed-in with tests for a feature that is to be removed.  That is, the tests for a system are not well decomposed, and some functionality is only tested in conjunction with a feature that a developer wants to ``adapt away.''  Fortunately, these problems can be addressed by taking a better approach to tests in general.

\subsection{Code Coverage} 
TBSM will remove any code that is not covered by any tests, and not required for compilation. The easiest way to avoid this problem is to start with a high-coverage, high-quality test suite. Developers can target poorly covered code with manual tests or use targeted test generation tools to cover such code. Improving code coverage using random testing reduced overfitting by 23\% and 8\% for \textit{Elevation API} and \textit{NetBeans IDE} respectively. Na\"ive test generation will not necessarily improve matters, however. Developers have to label any generated tests, and if tests mix multiple features, there still may often be cases where a feature not to be removed is never tested in isolation. We propose using delta-debugging, in particular \emph{cause reduction} to help isolate features in generated test cases~\cite{stvrcausereduce}: if a generated test covers two features, perhaps one can be removed by reducing the test with respect to targeted code for one feature only.  In fact, in principle, we can automatically take each individual line of code, and produce a tests whose only purpose is to cover that line of code, with any other behavior required to retain that coverage.  If developers can then label files or functions in a program by functionality, the targeted code's location can automatically provide a label.

\subsection{Better Tests} 
Based on our study (with the developers) of overfitting, we identified a few categories covering most of the incorrect removals. For the \textit{Elevation API}, there were no tests for logging, and so all {\tt LOG} statements were removed. This problem is likely very common, but can be easily remedied by explicitly testing system logging as a feature.  Testing the logging output for performed actions in the tests for the functionality is a seldom-performed, but likely beneficial practice.  Similarly, exception-handling code was often inadequately tested.

We worked with the developers to produce a new test covering each major overfitting category identified. We produced just three and four (relatively small) tests, respectively, for the \textit{Elevation API} and \textit{NetBeans IDE} scenarios, resulting in 20\% and 66\% reduction in overfitting. Identifying overfitting categories can be a highly context-specific task, but we suspect new automated test generation methods targeting specific under-tested categories such as logging and exception handling may be useful here.

\subsection{Motivating Better Tests}

Of course, the reason tests are inadequate in the first place is that developers did not produce extremely high-quality, fine-grained tests.  Developers often fail to test important functionality, perhaps on the grounds that it is ``only logging'' (despite the fact that logging is, in the long run, the only visibility and debugging aid for many systems).  More seriously, even developers of high-quality systems may not test certain exceptional behavior paths.  Targeting often-overlooked functionality is a good future goal for the automated testing community; perhaps logging code is not tested because developers expect to change logging output frequently in reponse to debugging needs, and don't want to change the tests.  Automated methods could generate (and re-generate) properly labeled tests just to test the logging output of existing unit or system tests, without making this a significant burden on developers.  Aggressive random testing and fuzzing is likely to expose untested exceptional scenarios.  Interestingly, TBSM can also be seen as a way to identify untested code:  just run TBSM on a code base with \emph{no} labeled tests.  Anything removed is a good candidate for additional testing!

But there is a larger picture here.  We suspect that developers do not put as much effort as TBSM would ideally expect into testing because the payoff of testing is not always clear.  The relationship between test quality, even measured by mutation testing rather than coarse-grained coverage only, and detection and anticipation of important defects worth fixing, is not always extremely strong~\cite{Testedness}.  However, there is clearly some relationship between test quality and eventual system reliability, and one way to see TBSM (and future techniques of the same kind) is as a new way to get better tests.  Because most code does not have a critical bug, effort to write tests is currently seen as a burden, a ``cost-center,'' not a ``revenue-center'' for developers, to adapt a business analogy.  It may have to be done, but testing is purely to ward off disaster.  However, TBSM offers a different way to look at tests:  high quality tests allow developers to be more productive, in that they enabled the \emph{automation of certain kinds of changes to the software.}  Writing high-quality tests may become a much more pleasant part of software development if the payoff is not having to write as much complex code to, e.g., adapt a system to a more limited platform, or handle changes to system libraries, or produce a more secure version with a smaller attack surface due to removing insecure functionalities.  If tests can produce ``productivity revenue'' rather than simply allowing the detection of faults in previous productivity, they may be more valued, and receive more attention.  The upshot will be more reliable systems that are also easier to adapt to resource-limited situations.